{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31t1_59ps4Z_"
      },
      "source": [
        "# 0. 개념의 이해\n",
        "\n",
        "나는 밥을 [ ] 에서 빈칸에 들어갈 말이 '먹는다'라는 것을 우리는 큰 고민 없이 알 수 있다. 밥은 통계적으로 먹는 것이고, 사람에 의해 먹히는 것이기 때문이다. 또 다른 예로 알바생이 커피를 [ ] 라면 아마도 만든다가 정답일 가능성이 높다. 알바생이 커피를 마실 수도 있지만, 통계적으론 만드니까!\n",
        "\n",
        "인공지능이 글을 이해하게 하는 방식도 위와 같다. 어떤 문법적인 원리를 통해서가 아니고, 수많은 글을 읽게 함으로써 나는, 밥을, 그 다음이 먹는다 라는 사실을 알게 하는 것이다. 그런 이유에서 많은 데이터가 곧 좋은 결과를 만들어 낼 수 있다. 단어를 적재적소에 활용하는 능력이 발달된다고 생각할 수 있다.\n",
        "\n",
        "이 방식을 가장 잘 처리하는 인공지능 중 하나가 순환신경망(RNN)으로, 이번 시간엔 자세한 내용보다는 간단한 구조를 중심으로 배우게 될 것이다.\n",
        "\n",
        "앞에서 먹었다 를 만드는 법은 배웠지만, 가장 첫 시작인 나는 은 어떻게 만들어야 할까? 이는 start 라는 특수한 토큰을 맨 앞에 추가해 줌으로써 해결할 수 있다. 인공지능에게 \"자, 이제 어떤 문장이든 생성해봐!\" 라는 사인을 주는 셈이다. start 를 입력으로 받은 순환신경망은 다음 단어로 나는 을 생성하고, 생성한 단어를 다시 입력으로 사용한다. 이 순환적인 특성을 살려 순환신경망이라고 이름을 붙인다!\n",
        "\n",
        "그렇게 순차적으로 밥을 먹었다 까지 생성하고 나면, 인공지능은 \"다 만들었어!\" 라는 사인으로 end 라는 특수한 토큰을 생성한다. 즉, 우리는 start 가 문장의 시작에 더해진 입력 데이터(문제지)와, end 가 문장의 끝에 더해진 출력 데이터(답안지)가 필요하며, 이는 문장 데이터만 있으면 만들어낼 수 있다는 것 또한 알 수 있다.\n",
        "\n",
        "위 과정을 파이썬으로는 아래와 같이 작성한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbwsmpI7s-ms",
        "outputId": "325cd573-7d89-4d74-b00c-1ca5903078c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source 문장: <start> 나는 밥을 먹었다 \n",
            "Target 문장:  나는 밥을 먹었다 <end>\n"
          ]
        }
      ],
      "source": [
        "sentence = \" 나는 밥을 먹었다 \"\n",
        "\n",
        "source_sentence = \"<start>\" + sentence\n",
        "target_sentence = sentence + \"<end>\"\n",
        "\n",
        "print(\"Source 문장:\", source_sentence)\n",
        "print(\"Target 문장:\", target_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JvHdRgdtF59"
      },
      "source": [
        "# 1. 필요한 라이브러리 및 파일 불러들이기!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhckwBZHtKnl",
        "outputId": "7e306b50-5f7c-489d-e164-44d3cd99cd88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "# 가장 먼저 tensorflow 라이브러리를 불러들이고 버전 확인!!\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tID2x2Hf_GZ",
        "outputId": "d2214360-d6d6-4da9-e15a-2deed697678f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os, re \n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 파일을 읽기모드로 열고\n",
        "# 라인 단위로 끊어서 list 형태로 읽어들임.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/AIFFEL/lyrics/*'\n",
        "txt_list = glob.glob(file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 더 쉽다.\n",
        "# lyrics 안에 있는 모든 txt 파일들을 읽은 후 raw_corpus 리스트에 문장 단위로 저장!!\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ng3bajqgQTs",
        "outputId": "4315dc36-59d8-49d4-c8fc-b2a6ccbc11d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for some education\n",
            "Made my way into the night\n",
            "All that bullshit conversation\n",
            "Baby, can't you read the signs? I won't bore you with the details, baby\n",
            "I don't even wanna waste your time\n",
            "Let's just say that maybe\n",
            "You could help me ease my mind\n",
            "I ain't Mr. Right But if you're looking for fast love\n",
            "If that's love in your eyes\n",
            "It's more than enough\n"
          ]
        }
      ],
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
        "\n",
        "    if idx > 9: break   # 문장 10개 확인\n",
        "        \n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrT_2tdDtUVY"
      },
      "source": [
        "# 2. 데이터 전처리(정제)\n",
        "\n",
        "입력된 문장을 전처리하는 과정을 거친다.\n",
        "\n",
        "1. 소문자로 바꾸고, 양쪽 공백을 지운다.\n",
        "2. 특수문자 양쪽에 공백을 넣고\n",
        "3. 여러개의 공백은 하나의 공백으로 바꾼다.\n",
        "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다.\n",
        "5. 다시 양쪽 공백을 지운다.\n",
        "6. 문장 시작에는 start, 끝에는 end를 추가해 준다.(편의상 <>는 생략)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MfeHuvolaxm",
        "outputId": "69129a7b-03cd-45f1-e870-cf4f14362a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()                         # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)         # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                 # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)       # 4\n",
        "    sentence = sentence.strip()                                 # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>'                 # 6\n",
        "    return sentence\n",
        "\n",
        "# 이 엉터리 문장이 어떻게 필터링되는지 확인해 보면!!!\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9AUrXDXtyBf"
      },
      "source": [
        "* coupus, 바로 여기에 정제된 문장을 모을 것이다.\n",
        "* 또한 문장을 단어 단위로 토큰화를 하는 과정을 거치게 될 것인데, 토큰의 개수가 15개를 넘어가는 문장을 제외하는 과정을 거치기 위해 splitdata라는 변수를 지정했다. 그리고 splitdata는 공백을 제외한 문장의 길이, 즉 단어의 개수가 나오게 된다.\n",
        "* 그 외에도 문장의 길이가 0인 문장 역시 건너뛴다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dmn038SldMt",
        "outputId": "23de6f83-a3a1-4cc2-a8da-dc65bb074b11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> i don t even wanna waste your time <end>',\n",
              " '<start> let s just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
              " '<start> if that s love in your eyes <end>',\n",
              " '<start> it s more than enough <end>',\n",
              " '<start> had some bad love <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#정제 데이터 구축하기\n",
        "corpus = []\n",
        "\n",
        "# 우리가 원하지 않는 문장 건너뜀\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue           # 길이 0\n",
        "    if len(sentence.split()) >= 13: continue  # 15개 이하(start,end포함)\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "    \n",
        "#정제된 결과 10개만 추려서 확인\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5hEcDvxus3o"
      },
      "source": [
        "* 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용할 것이다.\n",
        "* 12000단어를 기억할 수 있는 tokenizer를 만든다. 단어장의 크기를 12000 이상으로 설정하라고 했기 때문에 num_words는 12000으로 설정한 것이다.\n",
        "* 우리는 이미 문장을 정제했으니 filters가 필요 X\n",
        "* 12000단어에 포함되지 못한 단어는 unk로 바꾸기\n",
        "* corpus를 이용해 tokenizer 내부의 단어장을 완성한다.\n",
        "* 준비한 tokenizer를 이용해 corpus를 Tensor로 변환한다.\n",
        "* 입력 데이터의 시퀀스 길이를 일정하게 맞춘다.\n",
        "* 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰준다.\n",
        "* 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용하면 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1i8hzi5lfQT",
        "outputId": "32bff3b9-2403-4fd8-f701-c5283afc7d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 290  28 ...   0   0   0]\n",
            " [  2 223  13 ...   0   0   0]\n",
            " [  2  25  17 ...   0   0   0]\n",
            " ...\n",
            " [  2 263 192 ...   0   0   0]\n",
            " [  2 127   5 ...   0   0   0]\n",
            " [  2   7  36 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fba2cbf49d0>\n"
          ]
        }
      ],
      "source": [
        "#tokenize() 함수로 데이터를 Tensor로 변환\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, #단어장의 크기:12,000 이상 \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    \n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)  # corpus를 Tensor로 변환 \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfNmzj_lu3-b",
        "outputId": "966ef2ee-c4f1-4288-8006-a332911004bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  290   28   96 4536    3    0    0    0    0]\n",
            " [   2  223   13   87  224    6  115    3    0    0]\n",
            " [   2   25   17 1072 2290    3    0    0    0    0]]\n"
          ]
        }
      ],
      "source": [
        "print(tensor[:3, :10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuNKeVBQlhe1",
        "outputId": "64f755e8-ea9f-4018-9a55-356853be3f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n",
            "[   2  290   28   96 4536    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[ 290   28   96 4536    3    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "#단어사전 구축 인덱스 확인\n",
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break\n",
        "\n",
        "#생성된 텐서를 소스와 타겟으로 분리해 모델 학습\n",
        "src_input = tensor[:, :-1]   #소스 문장을 생성\n",
        "tgt_input = tensor[:, 1:]   #타겟 문장을 생성\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgqpuEfeu8nK"
      },
      "source": [
        "* tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성.\n",
        "* 마지막 토큰은 end가 아니라 pad일 가능성이 높을 것이다.\n",
        "* src_input을 훈련 문장 셋으로 구성하고, tensor에서 start를 잘라내서 타겟 문장(tgt_input)을 생성."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O65wyiWvlj8Q",
        "outputId": "bc1e49b6-9674-4d57-8e1c-8cda1460da9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 32), dtype=tf.int32, name=None), TensorSpec(shape=(256, 32), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#데이터셋 객체 생성\n",
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "# tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만든다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRdxuFF4y-Al"
      },
      "source": [
        "# 3. 평가 데이터셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LktiYQlXln3_",
        "outputId": "4d5b2dab-bb16-40c1-ccc0-0dd0f495a7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(127100, 32)\n",
            "(31776, 32)\n",
            "(127100, 32)\n"
          ]
        }
      ],
      "source": [
        "# 총 데이터의 20% 를 평가 데이터셋\n",
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 15)\n",
        "\n",
        "print(enc_train.shape)\n",
        "print(enc_val.shape)\n",
        "print(dec_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QgXEHCyxltsG"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "# 이번엔 이 embedding_size와 hidden_size를 하이퍼 파라미터로써 사용하게 될 것이다!!    \n",
        "embedding_size = 256     # 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
        "hidden_size = 1024       # 모델에 얼마나 많은 일꾼을 둘 것인가\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxWpCN8AlvaY",
        "outputId": "3067ff41-b690-4cb6-c0b0-a4dacd3d604e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n",
              "array([[[-1.63065575e-04, -2.27829543e-04, -7.44723002e-05, ...,\n",
              "         -4.18515207e-04, -1.27158972e-04,  3.54587479e-04],\n",
              "        [-3.96343850e-04, -4.33233276e-04, -2.94095837e-04, ...,\n",
              "         -6.80554425e-04, -1.94785796e-04,  7.86120421e-04],\n",
              "        [-5.45721618e-04, -6.63791550e-04, -3.54479038e-04, ...,\n",
              "         -1.27532787e-03, -2.76885927e-04,  9.73741873e-04],\n",
              "        ...,\n",
              "        [ 1.77094771e-03, -6.88493659e-04, -5.27805043e-03, ...,\n",
              "          1.73789740e-04, -1.63608952e-03, -7.56788766e-04],\n",
              "        [ 1.81339576e-03, -6.91739435e-04, -5.37482556e-03, ...,\n",
              "          1.62553493e-04, -1.72821013e-03, -7.74133019e-04],\n",
              "        [ 1.84905436e-03, -6.95305469e-04, -5.45659941e-03, ...,\n",
              "          1.50050648e-04, -1.80373504e-03, -7.87228637e-04]],\n",
              "\n",
              "       [[-1.63065575e-04, -2.27829543e-04, -7.44723002e-05, ...,\n",
              "         -4.18515207e-04, -1.27158972e-04,  3.54587479e-04],\n",
              "        [-3.63515981e-04, -6.15823432e-04,  1.46162725e-04, ...,\n",
              "         -7.48171238e-04, -3.42610903e-04,  3.86165950e-04],\n",
              "        [-1.39620155e-04, -6.01862674e-04,  1.88299702e-04, ...,\n",
              "         -5.78156149e-04, -3.82260216e-04,  2.83771544e-04],\n",
              "        ...,\n",
              "        [ 1.89109053e-03, -7.40570482e-04, -5.70751587e-03, ...,\n",
              "          1.41266821e-04, -1.97409117e-03, -7.52606720e-04],\n",
              "        [ 1.91324041e-03, -7.39583804e-04, -5.73914964e-03, ...,\n",
              "          1.24283848e-04, -1.98955904e-03, -7.59285002e-04],\n",
              "        [ 1.93295558e-03, -7.38686009e-04, -5.76535054e-03, ...,\n",
              "          1.09425055e-04, -2.00037169e-03, -7.65097793e-04]],\n",
              "\n",
              "       [[-1.63065575e-04, -2.27829543e-04, -7.44723002e-05, ...,\n",
              "         -4.18515207e-04, -1.27158972e-04,  3.54587479e-04],\n",
              "        [-3.21103522e-04, -5.30924241e-04, -3.52861767e-04, ...,\n",
              "         -5.47813426e-04, -4.02858830e-04,  4.98768582e-04],\n",
              "        [-2.93976307e-04, -5.78091189e-04, -8.87167407e-04, ...,\n",
              "         -5.28264092e-04, -5.44958166e-04,  4.53463726e-04],\n",
              "        ...,\n",
              "        [ 1.98376738e-03, -6.74850075e-04, -5.77119133e-03, ...,\n",
              "          1.28531537e-04, -1.96594396e-03, -7.80191098e-04],\n",
              "        [ 1.99480657e-03, -6.81026548e-04, -5.79150673e-03, ...,\n",
              "          1.10826288e-04, -1.97993242e-03, -7.84921402e-04],\n",
              "        [ 2.00446462e-03, -6.86929445e-04, -5.80832316e-03, ...,\n",
              "          9.59748359e-05, -1.98976183e-03, -7.88710546e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-1.63065575e-04, -2.27829543e-04, -7.44723002e-05, ...,\n",
              "         -4.18515207e-04, -1.27158972e-04,  3.54587479e-04],\n",
              "        [-2.21334529e-04, -4.04984952e-04, -2.14599728e-04, ...,\n",
              "         -7.24504644e-04, -3.28137889e-04,  6.78684912e-04],\n",
              "        [-1.46668783e-04, -4.70458763e-04,  6.65623083e-05, ...,\n",
              "         -9.46319429e-04, -4.45343263e-04,  8.05283722e-04],\n",
              "        ...,\n",
              "        [ 1.91293645e-03, -6.36199838e-04, -5.65902051e-03, ...,\n",
              "          1.26683983e-04, -2.09075771e-03, -8.35444778e-04],\n",
              "        [ 1.93176523e-03, -6.47191133e-04, -5.69703942e-03, ...,\n",
              "          1.16965894e-04, -2.09945161e-03, -8.34184757e-04],\n",
              "        [ 1.94855779e-03, -6.57039927e-04, -5.72889438e-03, ...,\n",
              "          1.07213855e-04, -2.10269680e-03, -8.31889687e-04]],\n",
              "\n",
              "       [[-1.63065575e-04, -2.27829543e-04, -7.44723002e-05, ...,\n",
              "         -4.18515207e-04, -1.27158972e-04,  3.54587479e-04],\n",
              "        [-1.99204354e-04, -2.70001474e-04, -2.40823822e-04, ...,\n",
              "         -5.25216747e-04, -1.43602738e-04,  4.34442307e-04],\n",
              "        [-2.71995814e-04, -5.27009892e-04, -5.01817733e-04, ...,\n",
              "         -6.93530543e-04, -3.32153904e-05,  5.16026572e-04],\n",
              "        ...,\n",
              "        [ 1.86105061e-03, -6.63366809e-04, -5.60521614e-03, ...,\n",
              "          2.01070899e-04, -1.95225922e-03, -8.22019065e-04],\n",
              "        [ 1.89466844e-03, -6.75073243e-04, -5.65392431e-03, ...,\n",
              "          1.80439354e-04, -1.98054733e-03, -8.19730281e-04],\n",
              "        [ 1.92313944e-03, -6.84494094e-04, -5.69466595e-03, ...,\n",
              "          1.61010859e-04, -2.00095866e-03, -8.17458727e-04]],\n",
              "\n",
              "       [[-1.63065575e-04, -2.27829543e-04, -7.44723002e-05, ...,\n",
              "         -4.18515207e-04, -1.27158972e-04,  3.54587479e-04],\n",
              "        [-2.58598389e-04, -5.93161531e-05, -4.65025551e-05, ...,\n",
              "         -5.18817047e-04, -4.01997706e-04,  9.02382191e-04],\n",
              "        [-1.03881641e-04,  1.43834870e-04, -1.98136928e-04, ...,\n",
              "         -4.66244441e-04, -6.30070223e-04,  1.13695022e-03],\n",
              "        ...,\n",
              "        [ 1.83892751e-03, -6.76849391e-04, -5.51054301e-03, ...,\n",
              "          2.71021447e-04, -1.95506471e-03, -8.54671118e-04],\n",
              "        [ 1.87286362e-03, -6.84921513e-04, -5.57557214e-03, ...,\n",
              "          2.42548544e-04, -1.98171893e-03, -8.52400262e-04],\n",
              "        [ 1.90186058e-03, -6.91732450e-04, -5.62997721e-03, ...,\n",
              "          2.15846361e-04, -2.00148975e-03, -8.49431846e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오기\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6yBVZWhlw3X",
        "outputId": "11a0be53-e42b-4b8b-b940-5afb042de81e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  3072256   \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  12301025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,012,961\n",
            "Trainable params: 29,012,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델을 학습할 시간!!! 파라미터를 보면 알겠지만 무려 3000만개에 육박한다. 모델 학습하는 게 시간이 걸린다는 것을 알 수 있다. 아무리 빨라도 최소 15분 정도 걸린다고 하니 여유를 가지고 작업하자!!!  \n",
        "\n",
        "model.fit에는 다양한 인자를 넣을 수 있다고 한다. 자세한 내용은 다음 사이트를 참고하면 좋을 듯 싶다.  \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit "
      ],
      "metadata": {
        "id": "Fpgxa_kd0q63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWw13WL1l0T4",
        "outputId": "85fce349-146e-4148-9c23-799705770d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "497/497 [==============================] - 122s 236ms/step - loss: 1.6420 - val_loss: 1.4055\n",
            "Epoch 2/10\n",
            "497/497 [==============================] - 117s 235ms/step - loss: 1.3589 - val_loss: 1.3311\n",
            "Epoch 3/10\n",
            "497/497 [==============================] - 116s 233ms/step - loss: 1.2873 - val_loss: 1.2808\n",
            "Epoch 4/10\n",
            "497/497 [==============================] - 116s 233ms/step - loss: 1.2325 - val_loss: 1.2419\n",
            "Epoch 5/10\n",
            "497/497 [==============================] - 116s 234ms/step - loss: 1.1845 - val_loss: 1.2116\n",
            "Epoch 6/10\n",
            "497/497 [==============================] - 117s 235ms/step - loss: 1.1441 - val_loss: 1.1865\n",
            "Epoch 7/10\n",
            "497/497 [==============================] - 116s 234ms/step - loss: 1.1014 - val_loss: 1.1655\n",
            "Epoch 8/10\n",
            "497/497 [==============================] - 119s 240ms/step - loss: 1.0634 - val_loss: 1.1482\n",
            "Epoch 9/10\n",
            "497/497 [==============================] - 117s 235ms/step - loss: 1.0277 - val_loss: 1.1322\n",
            "Epoch 10/10\n",
            "497/497 [==============================] - 116s 234ms/step - loss: 0.9944 - val_loss: 1.1198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb9b7538490>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, \n",
        "          dec_train, \n",
        "          epochs=10,\n",
        "          batch_size=256,\n",
        "          validation_data=(enc_val, dec_val),\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7ahJeV4Sl4m8"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    while True:\n",
        "        # 입력받은 문장의 텐서를 입력\n",
        "        predict = model(test_tensor) \n",
        "        # 예측된 값 중 가장 높은 확률인 word index를 뽑기\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 예측된 word index를 문장 뒤에 붙임\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마침\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rRjeWWxCv0nO",
        "outputId": "6bd85709-e746-4fb3-d39f-68279d21a1c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you , i m not gonna be <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "#문장 생성 함수 실행\n",
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPuxgn2SzLkw"
      },
      "source": [
        "# 회고\n",
        "\n",
        "이번 시간에는 자연어 처리 기법을 통해 문장 내 단어들을 토큰화하고 단어에 추상적 의미를 부여해 주는 워드 벡터로 바꿔주는 embedding 레이어가 포함된 모델을 설계하여 원하는 가사 한 줄을 써 보는 실습을 하게 되었다.  \n",
        "\n",
        "처음 할 때는 주피터 노트북을 이용해서 실행을 하였지만, 똑같이 코드를 작성하였음에도 계속 원인을 모를 오류가 걸리고 주피터 노트북에서 batch_size를 조금 낮춰가며 엄청나게 많은 데이터셋의 수용력을 조금 늘려서 훈련 속도를 빠르게 해 줘 봤지만 터무니없이 느려서 혼돈이 오기도 했다.  \n",
        "\n",
        "결국 이번 노드는 코랩을 통해서 실행하게 되었고, 정상적으로 코드가 잘 돌아갔으며 결과도 원하는 대로 나올 수 있었다. 좀 더 공부해서 주피터 노트북으로 했을 때 발생했던 문제점에 대해 해결해 보기도 하며 문제 해결 능력을 더 향상시키고자 한다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Exploration_06_김건희",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}