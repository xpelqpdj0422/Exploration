{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploration_10 김건희",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0. 필요한 라이브러리 및 모듈 불러오기, 파일 다운로드!!!\n",
        "\n",
        "이번 프로젝트는 Word-Level 번역기를 만드는 과정을 담은 해당 블로그를 상당 부분 참고하였다. 처음 보는 형태의 모듈이나 기타 의문점이 드는 사항들은 주석을 달아가며 작성해 봤다!!!  \n",
        "참고 링크 : https://wikidocs.net/86900"
      ],
      "metadata": {
        "id": "gFdxJgppmhNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C-aNx6yfUbI",
        "outputId": "bd764a9c-edea-494f-b91e-1f566abbd0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import os          # 개별 파일에 대한 연산 모듈\n",
        "import re\n",
        "import shutil      # 파일 모음에 대한 여러 가지 고수준 연산을 제공하는 기능의 모듈\n",
        "import zipfile     # 압축 파일에 대한 연산 모듈\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unicodedata     # 유니코드 문자에 대한 문자 속성을 정의하는 유니코드 문자 데이터베이스에 대한 액세스 제공!!\n",
        "import urllib3         # 파이썬용 강력하고 사용자 친화적인 HTTP 클라이언트. gzip, deflate 및 brotli 인코딩 지원 및 기타 다양한 기능 제공!!\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "처음으로 접하는 모듈이 몇 가지가 있어 간략하게 알아보자!!! 링크를 통해 해당 모듈이 어떤 기능을 하는지 위주로 챙겨보기!!  \n",
        "* shutil : https://docs.python.org/ko/3/library/shutil.html  \n",
        "* unicodedata : https://docs.python.org/ko/3/library/unicodedata.html  \n",
        "* urlib3 : https://urllib3.readthedocs.io/en/stable/"
      ],
      "metadata": {
        "id": "cZCXGSFch6NA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "urllib3랑 zipfile 등의 모듈을 활용해 url 상의 fra-eng.zim 파일을 내려받고 압축 파일을 풀어보는 과정을 진행하였다. "
      ],
      "metadata": {
        "id": "V-Ju5ltunMnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "http = urllib3.PoolManager()\n",
        "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
        "  shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "  zip_ref.extractall(path)"
      ],
      "metadata": {
        "id": "S3HI8cS9feyh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제로 앞선 과정을 밟아오면서 우리가 사용했던 그 파일을 다운받은 것이지만, 아까와는 달리 인터넷 상에서의 파일을 코랩에서 바로 다운받는 코드를 직접 작성하고 압축 파일 해제까지를 완료한 상태다. 때문에 현재 해당 데이터셋이 이제 코랩에서 정상적으로 인지할 수 있는 상태가 되었다. fra.txt에 있는 파일을 pandas를 통해 읽어들여서 한번 샘플 확인을 해보자!!"
      ],
      "metadata": {
        "id": "k8OfRGNY7i-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moOie0wh6cQ7",
        "outputId": "fb251e6b-7dd7-469c-ab0f-2b53960f1bb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/AIFFEL/DATASET/fra.txt'\n",
        "lines =  pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
        "print('전체 샘플의 수 :',len(lines))\n",
        "lines.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "rn4lC4Hj7Dd0",
        "outputId": "8efabace-3906-4f9a-9944-6eef3dfd80e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플의 수 : 194513\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                      eng  \\\n",
              "183290  Mary was a tomboy when she was in elementary s...   \n",
              "87960                         Keep your eyes on the road.   \n",
              "181011  It seems like the cat caught the scent of a mo...   \n",
              "43688                               It's still too early.   \n",
              "177380    Why don't you start by telling us how you feel?   \n",
              "\n",
              "                                                      fra  \\\n",
              "183290  Mary était un garçon manqué quand elle était à...   \n",
              "87960                       Garde les yeux sur la route !   \n",
              "181011  On dirait que le chat a détecté l'odeur d'une ...   \n",
              "43688                             Il est encore trop tôt.   \n",
              "177380  Pourquoi ne commences-tu pas par nous dire ce ...   \n",
              "\n",
              "                                                       cc  \n",
              "183290  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
              "87960   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "181011  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "43688   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
              "177380  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-699edef4-3083-4e60-87ca-52c600060157\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fra</th>\n",
              "      <th>cc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>183290</th>\n",
              "      <td>Mary was a tomboy when she was in elementary s...</td>\n",
              "      <td>Mary était un garçon manqué quand elle était à...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87960</th>\n",
              "      <td>Keep your eyes on the road.</td>\n",
              "      <td>Garde les yeux sur la route !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181011</th>\n",
              "      <td>It seems like the cat caught the scent of a mo...</td>\n",
              "      <td>On dirait que le chat a détecté l'odeur d'une ...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43688</th>\n",
              "      <td>It's still too early.</td>\n",
              "      <td>Il est encore trop tôt.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177380</th>\n",
              "      <td>Why don't you start by telling us how you feel?</td>\n",
              "      <td>Pourquoi ne commences-tu pas par nous dire ce ...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-699edef4-3083-4e60-87ca-52c600060157')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-699edef4-3083-4e60-87ca-52c600060157 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-699edef4-3083-4e60-87ca-52c600060157');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞선 과정과 동일하게 영어-불어 19만개의 데이터들이 담긴 데이터셋을 활용하는 것임을 재확인했다. 이번에는 프로젝트 실행 조건에 맞게끔 데이터 개수를 33000개로 한정지었다. 다음 코드를 작성하면 데이터 사용 개수를 한정지을 수 있다."
      ],
      "metadata": {
        "id": "aTlBuPleplS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터에서 상위 33,000개의 샘플만 사용하도록 데이터 개수를 한정지었다.\n",
        "num_samples = 33000\n",
        "lines = lines[['eng', 'fra']][:33000]\n",
        "print('전체 샘플의 수 :',len(lines))\n",
        "lines.sample(5)"
      ],
      "metadata": {
        "id": "wfrsnJe0nlAZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b043c986-7d44-4c72-ce73-323cb2d3e666"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플의 수 : 33000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      eng              fra\n",
              "15082    Just stay there.       Reste ici.\n",
              "26162  This is messed up.     C'est foutu.\n",
              "11366     Make it larger.   Élargissez-le.\n",
              "1660          Are you in?  Es-tu partant ?\n",
              "700             It's odd.   C'est bizarre."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebceb7dc-908a-4f19-a977-40b4a46df412\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15082</th>\n",
              "      <td>Just stay there.</td>\n",
              "      <td>Reste ici.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26162</th>\n",
              "      <td>This is messed up.</td>\n",
              "      <td>C'est foutu.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11366</th>\n",
              "      <td>Make it larger.</td>\n",
              "      <td>Élargissez-le.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>Are you in?</td>\n",
              "      <td>Es-tu partant ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>700</th>\n",
              "      <td>It's odd.</td>\n",
              "      <td>C'est bizarre.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebceb7dc-908a-4f19-a977-40b4a46df412')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ebceb7dc-908a-4f19-a977-40b4a46df412 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ebceb7dc-908a-4f19-a977-40b4a46df412');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. 정제, 정규화, 전처리(영어, 불어 모두)\n",
        "\n",
        "글자 단위가 아닌 단어 단위의 번역기를 만드는 것이 목표이기 때문에 글자 단위에서 신경쓰지 않았던 몇 가지 추가적인 전처리 작업이 필요하다.\n",
        "\n",
        "단어라고 함은 띄어쓰기를 기준으로 분리하는데,  어디서부터 어디까지가 하나의 단어인지를 명확히 해야만 한다.\n",
        "\n",
        "또한 구두점을 분리하는 작업도 필요하다. 이를 신경 쓰지 않는다면 단어 뒤에 붙어있는 !나 ?나 . 같은 구두점을 포함한 채로 토큰화가 진행되게 된다. 하지만 실제로 구두점은 어떤 단어와 붙어있는 한 단어가 아니기 때문에 분리를 해주는 것이다.\n",
        "\n",
        "또한 영어와 달리 프랑스어의 경우는 아래 링크를 통해 알 수 있듯이 악쌍이 붙어 있는 단어들이 많이 있다. 이 악쌍 역시 제거해 줘야 나중에 번역기를 돌릴 때 이상 없이 나올 수 있을 것이기 때문에 미리 없애는 작업을 진행한다.\n",
        "프랑스어의 악쌍(accents) 관련 참조 링크 : https://in2youruniverse.tistory.com/9"
      ],
      "metadata": {
        "id": "-leav4fUmeuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 불어의 경우는 악센트가 붙어 있는 철자가 상당수 존재한다. 이 악센트를 삭제해야 한다.\n",
        "# 예시 : 'déjà diné' -> deja dine\n",
        "def to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "  # 악센트 제거 함수 호출\n",
        "  sent = to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백 추가.\n",
        "  # 띄어쓰기 단위로 토큰화를 수행해야 하기 때문에 이런 분리작업이 필요하다!!\n",
        "  # ex) \"I am a student.\" => \"I am a student .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "Rhys_oN3sJ75"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"I really need to go outside and get some fresh...\"\n",
        "fr_sent = u\"As-tu déjà été diagnostiquée séropositive?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 불어 문장 :', fr_sent)\n",
        "print('전처리 후 불어 문장 :', preprocess_sentence(fr_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDxAAN1lguSJ",
        "outputId": "2a3d3f89-70fd-498c-cbd3-da0e9871a377"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : I really need to go outside and get some fresh...\n",
            "전처리 후 영어 문장 : i really need to go outside and get some fresh . . .\n",
            "전처리 전 불어 문장 : As-tu déjà été diagnostiquée séropositive?\n",
            "전처리 후 불어 문장 : as tu deja ete diagnostiquee seropositive ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 데이터에서 33,000개의 샘플에 대해서 전처리를 수행한다. 입력 시퀀스에는 시작을 의미하는 토큰인 `<sos>`를 추가하고, 출력 시퀀스에는 종료를 의미하는 토큰인 `<eos>`를 추가한다."
      ],
      "metadata": {
        "id": "Dse-49Ao9SAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. 디코더의 문장에 시작 토큰과 종료 토큰 삽입\n",
        "\n",
        "글자 단위 번역기를 구현할 때와 마찬가지로 입력 시퀀스 맨 앞에는 시작을 의미하는 토큰이 필요하다.  \n",
        "예를 들어 번역 문장이 Courez!이었다면 이 문자에 대해 각각 디코더의 입력 시퀀스와 레이블 시퀀스를 만들면 다음과 같다.\n",
        "\n",
        "입력 시퀀스 : ['', 'courez', '!']\n",
        "레이블 시퀀스 : ['courez','!', '']"
      ],
      "metadata": {
        "id": "K1SWRuj9tAmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      # 불어 입력 문장 앞에는 <sos> 토큰을, 출력 문장 뒤에는 <eos> 토큰을 첨가한다는 뜻!!\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "DHIVwkFjhGcg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더가 되는 영어 문장과 디코더가 되는 문장인 불어 문장의 인덱스를 임의로 가져와 토큰화가 잘 되었는지 확인해 보자!!!"
      ],
      "metadata": {
        "id": "VjZm59ZpFHam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0부터 32999까지의 숫자 중 임의로 삽입하여 각 문장마다 토큰화가 잘 되었는지 확인해 보는 묘미를~~\n",
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[29000:29005])\n",
        "print('디코더의 입력 :',sents_fra_in[29000:29005])\n",
        "print('디코더의 레이블 :',sents_fra_out[29000:29005])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JO1V6xG-kcd",
        "outputId": "534501bd-612d-40b6-d813-51312de5a312"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력 : [['he', 's', 'in', 'the', 'shower', '.'], ['he', 's', 'my', 'new', 'friend', '.'], ['he', 's', 'not', 'a', 'bad', 'boy', '.'], ['he', 's', 'not', 'a', 'bad', 'guy', '.'], ['he', 's', 'not', 'all', 'there', '.']]\n",
            "디코더의 입력 : [['<sos>', 'il', 'est', 'dans', 'la', 'douche', '.'], ['<sos>', 'c', 'est', 'mon', 'nouvel', 'ami', '.'], ['<sos>', 'ce', 'n', 'est', 'pas', 'un', 'mauvais', 'garcon', '.'], ['<sos>', 'ce', 'n', 'est', 'pas', 'un', 'mauvais', 'bougre', '.'], ['<sos>', 'il', 'n', 'est', 'pas', 'tout', 'a', 'fait', 'la', '.']]\n",
            "디코더의 레이블 : [['il', 'est', 'dans', 'la', 'douche', '.', '<eos>'], ['c', 'est', 'mon', 'nouvel', 'ami', '.', '<eos>'], ['ce', 'n', 'est', 'pas', 'un', 'mauvais', 'garcon', '.', '<eos>'], ['ce', 'n', 'est', 'pas', 'un', 'mauvais', 'bougre', '.', '<eos>'], ['il', 'n', 'est', 'pas', 'tout', 'a', 'fait', 'la', '.', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꾸기\n",
        "\n",
        "딥 러닝 모델은 각 단어를 텍스트가 아닌 숫자를 처리한다. 케라스 토크나이저를 사용해서 각 단어를 고유한 정수 형태로 바꾸는 작업이 필요하다!!"
      ],
      "metadata": {
        "id": "jBgXBfyuuDkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "encoder_input = pad_sequences(encoder_input, padding='post')\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding='post')\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding='post')"
      ],
      "metadata": {
        "id": "BKNQ1K3uBH9T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더의 입력의 크기(shape) :', encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :', decoder_input.shape)\n",
        "print('디코더의 출력의 크기(shape) :', decoder_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLHDrOjQv2Gm",
        "outputId": "641b037a-e1d6-422d-d899-328147b187d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력의 크기(shape) : (33000, 8)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 출력의 크기(shape) : (33000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "샘플은 총 33,000개 존재하며 영어 문장의 길이는 8, 불어 문장의 길이는 16인 것을 알 수 있다. 이제 단어 집합의 크기를 정의할 차례이다."
      ],
      "metadata": {
        "id": "hvjM3H0YwSam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 불어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnKDUrvdwQIJ",
        "outputId": "6e0ab7aa-ae08-451d-e9ce-9f86d85e42ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어 집합의 크기 : 4672, 불어 단어 집합의 크기 : 8153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 집합의 크기는 각각 영어 4672개, 불어 8153개이다. 단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 생성하도록 한다. 이들은 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용될 것이다."
      ],
      "metadata": {
        "id": "1hbbENN21EMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word"
      ],
      "metadata": {
        "id": "mjire-jcxruL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 데이터를 분리하기 전 데이터를 섞어준다. 아무래도 실질적인 번역기의 성능을 확인해 보기 위해 무작위로 자료를 섞는 작업이라고 생각했다. 먼저 순서가 섞인 정수 시퀀스 리스트(indices)를 만들어 주고, 데이터를 섞을 때는 np.random.shuffle 함수를 활용하면 된다."
      ],
      "metadata": {
        "id": "8oOmLkLh17E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 시퀀스 리스트(indices) 생성\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('랜덤 시퀀스 :', indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1t9DtD01pcY",
        "outputId": "64f89e54-1593-41ce-fdcc-5897ec018e71"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시퀀스 : [30974 29819 32075 ...  2139 10944 17036]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "XDP9dle_13d3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의로 샘플 출력(0부터 32999까지 중 아무 숫자나 넣어보면 됨)\n",
        "encoder_input[30005]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDI_t-Vx2NPQ",
        "outputId": "9ed60392-e51a-4a76-fd76-0d404b1675bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2, 839,   3,   1,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의로 샘플 출력(0부터 32999까지 중 아무 숫자나 넣어보면 됨)\n",
        "decoder_input[30005]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN4NfTck3bcX",
        "outputId": "535b6b8a-af19-4638-bbe3-0467760565be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,   4,  44,  15, 492,   1,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의로 샘플 출력(0부터 32999까지 중 아무 숫자나 넣어보면 됨)\n",
        "decoder_target[30005]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysP4oYZY3bjg",
        "outputId": "b6d50283-1154-446f-af65-33955de5a5cd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  4,  44,  15, 492,   1,   3,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 때 decoder_input과 decoder_target은 데이터 구조상으로 앞에 붙은 `<sos>` 토큰과 뒤에 붙은 `<eos>` 토큰을 제외하면 동일한 정수 시퀀스를 가져야 한다. 같은 문장이기 때문에 동일한 정수 시퀀스를 가져야 될 것으로 여겨진다.\n",
        "\n",
        "이 말은 결과를 보면 확연히 알 수 있는데, `decoder_input`의 2가 바로 `<sos>` 토큰을 의미하는 것이고, `decoder_output` 결과 내 3이 바로 `<eos>` 토큰을 의미하는 것이다. 그리고 이 두 개의 숫자를 제외한 나머지 숫자의 시퀀스를 비교해 보면 [11, 25, 77, 1822, 1]로 일치하는 것을 확인할 수 있었고 위의 개념을 직관적으로 이해할 수 있는 대목이라고 할 수 있다!!!!"
      ],
      "metadata": {
        "id": "uGMSDYJl27RY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제부터는 33,000개의 데이터들 중 3,000개의 데이터를 테스트 데이터로써 사용한다!!"
      ],
      "metadata": {
        "id": "nlpe-_5L50pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(33000/11)\n",
        "print('검증 데이터의 개수 :',n_of_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCN1KemH2tpY",
        "outputId": "e4dd4dc0-e27e-4504-b25c-0ea75e4d0f60"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 데이터의 개수 : 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터 설정\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "# 테스트 데이터 설정\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "-QjZ1zb-5yUI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터와 테스트 데이터의 크기 출력!!\n",
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqWYknwF6Shf",
        "outputId": "c23931a6-2b01-453f-8fe1-cf3b7a1a4bdf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 source 데이터의 크기 : (30000, 8)\n",
            "훈련 target 데이터의 크기 : (30000, 16)\n",
            "훈련 target 레이블의 크기 : (30000, 16)\n",
            "테스트 source 데이터의 크기 : (3000, 8)\n",
            "테스트 target 데이터의 크기 : (3000, 16)\n",
            "테스트 target 레이블의 크기 : (3000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. 임베딩 층 사용 및 모델 구현"
      ],
      "metadata": {
        "id": "MCiGoP4J6omC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "KHcfZjaF6Wr3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64로 설정한다."
      ],
      "metadata": {
        "id": "YyFssf8F7WC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "hidden_units = 64"
      ],
      "metadata": {
        "id": "jHU6Narl7Uni"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 인코더를 본격적으로 설계하고자 한다. 인코더를 주목해보면 함수형 API를 사용한다는 것 외에는 앞서 다른 실습에서 본 LSTM 설계와 크게 다르지는 않다. Masking은 패딩 토큰인 숫자 0의 경우에는 연산을 제외하는 역할을 수행한다. 인코더의 내부 상태를 디코더로 넘겨줘야 하기 때문에 return_state=True로 설정한다.\n",
        "\n",
        "LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 RNN 챕터에서 LSTM을 처음 설명할 때 언급하였던 은닉 상태와 셀 상태에 해당된다. 이 두 가지 상태를 encoder_states에 저장한다. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달할 것이다."
      ],
      "metadata": {
        "id": "rAkPr6dt7jKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 설계\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs)  # 임베딩 층\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb)   # 패딩 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True)  # 상태값 리턴을 위해 return_state는 True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)  # 은닉 상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c]   # 인코더의 은닉 상태와 셀 상태 저장"
      ],
      "metadata": {
        "id": "eOHJ25oT7emf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더는 인코더의 마지막 은닉 상태로부터 초기 은닉 상태를 얻는다. initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당된다. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않는다. seq2seq의 디코더는 기본적으로 각 시점마다 다중 클래스 분류 문제를 풀고 있다. 매 시점마다 프랑스어 단어 집합의 크기의 선택지에서 단어를 1개 선택해 이를 이번 시점에서 예측한 단어로 택한다. 다중 클래스 분류 문제이므로 출력층으로 소프트맥스 함수와 손실 함수를 크로스 엔트로피 함수를 사용한다.\n",
        "\n",
        "categorical_crossentropy를 사용하려면 레이블은 원-핫 인코딩이 된 상태여야 한다. 그런데 현재 decoder_outputs의 경우에는 원-핫 인코딩을 하지 않은 상태이다. 원-핫 인코딩을 하지 않은 상태로 정수 레이블에 대해서 다중 클래스 분류 문제를 풀고자 하는 경우에는 categorical_crossentropy가 아니라 sparse_categorical_crossentropy를 사용하면 된다.\n",
        "\n",
        "다중 클래스 손실 함수 관련 링크 : https://crazyj.tistory.com/153"
      ],
      "metadata": {
        "id": "rBx0N3tv9Mk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 설계\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)"
      ],
      "metadata": {
        "id": "g6ol6wz89CvD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
        "\n",
        "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# 모델의 입력과 출력을 정의.\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "t4VhJCeI-be-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델을 훈련시킬 것이다. 128개 배치 크기로 총 50 에포크의 학습을 진행한다. 테스트 데이터를 검증 데이터로 사용해 훈련이 제대로 되고 있는지 확인해보자!!"
      ],
      "metadata": {
        "id": "aoNdvK-F_loV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=128, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N6KJtpQ_k8D",
        "outputId": "608f79c4-2a11-4fbc-93d2-dd58fc24482a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 26s 46ms/step - loss: 3.4086 - acc: 0.6117 - val_loss: 2.0788 - val_acc: 0.6158\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 1.8969 - acc: 0.6536 - val_loss: 1.7898 - val_acc: 0.7029\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 1.6885 - acc: 0.7318 - val_loss: 1.6250 - val_acc: 0.7489\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 1.5451 - acc: 0.7558 - val_loss: 1.4995 - val_acc: 0.7620\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 8s 36ms/step - loss: 1.4285 - acc: 0.7674 - val_loss: 1.4007 - val_acc: 0.7716\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 1.3273 - acc: 0.7824 - val_loss: 1.3067 - val_acc: 0.7919\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 1.2343 - acc: 0.8002 - val_loss: 1.2342 - val_acc: 0.8062\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 1.1653 - acc: 0.8109 - val_loss: 1.1835 - val_acc: 0.8131\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 1.1103 - acc: 0.8180 - val_loss: 1.1426 - val_acc: 0.8195\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 8s 36ms/step - loss: 1.0615 - acc: 0.8247 - val_loss: 1.1064 - val_acc: 0.8235\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 1.0170 - acc: 0.8300 - val_loss: 1.0732 - val_acc: 0.8275\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 8s 36ms/step - loss: 0.9765 - acc: 0.8343 - val_loss: 1.0460 - val_acc: 0.8306\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 9s 40ms/step - loss: 0.9409 - acc: 0.8381 - val_loss: 1.0217 - val_acc: 0.8336\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.9083 - acc: 0.8416 - val_loss: 1.0020 - val_acc: 0.8362\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.8784 - acc: 0.8446 - val_loss: 0.9827 - val_acc: 0.8380\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 0.8504 - acc: 0.8479 - val_loss: 0.9654 - val_acc: 0.8398\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.8241 - acc: 0.8500 - val_loss: 0.9501 - val_acc: 0.8419\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 8s 36ms/step - loss: 0.7997 - acc: 0.8526 - val_loss: 0.9349 - val_acc: 0.8439\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.7760 - acc: 0.8549 - val_loss: 0.9234 - val_acc: 0.8446\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.7530 - acc: 0.8576 - val_loss: 0.9093 - val_acc: 0.8467\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 0.7307 - acc: 0.8600 - val_loss: 0.8979 - val_acc: 0.8473\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.7092 - acc: 0.8621 - val_loss: 0.8865 - val_acc: 0.8489\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.6879 - acc: 0.8645 - val_loss: 0.8747 - val_acc: 0.8497\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.6679 - acc: 0.8670 - val_loss: 0.8652 - val_acc: 0.8514\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.6487 - acc: 0.8691 - val_loss: 0.8559 - val_acc: 0.8520\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.6300 - acc: 0.8714 - val_loss: 0.8469 - val_acc: 0.8533\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.6121 - acc: 0.8737 - val_loss: 0.8388 - val_acc: 0.8541\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 10s 41ms/step - loss: 0.5952 - acc: 0.8760 - val_loss: 0.8329 - val_acc: 0.8548\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.5783 - acc: 0.8780 - val_loss: 0.8257 - val_acc: 0.8549\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.5628 - acc: 0.8800 - val_loss: 0.8201 - val_acc: 0.8555\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 0.5477 - acc: 0.8822 - val_loss: 0.8149 - val_acc: 0.8565\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.5325 - acc: 0.8842 - val_loss: 0.8080 - val_acc: 0.8578\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.5184 - acc: 0.8864 - val_loss: 0.8030 - val_acc: 0.8581\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.5050 - acc: 0.8885 - val_loss: 0.7995 - val_acc: 0.8593\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 0.4918 - acc: 0.8905 - val_loss: 0.7953 - val_acc: 0.8602\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 0.4793 - acc: 0.8924 - val_loss: 0.7891 - val_acc: 0.8598\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.4671 - acc: 0.8945 - val_loss: 0.7890 - val_acc: 0.8604\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.4554 - acc: 0.8964 - val_loss: 0.7835 - val_acc: 0.8620\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.4438 - acc: 0.8981 - val_loss: 0.7809 - val_acc: 0.8617\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.4329 - acc: 0.9000 - val_loss: 0.7778 - val_acc: 0.8624\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.4224 - acc: 0.9018 - val_loss: 0.7767 - val_acc: 0.8628\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.4120 - acc: 0.9039 - val_loss: 0.7731 - val_acc: 0.8637\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 9s 37ms/step - loss: 0.4020 - acc: 0.9055 - val_loss: 0.7721 - val_acc: 0.8643\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 9s 40ms/step - loss: 0.3924 - acc: 0.9073 - val_loss: 0.7704 - val_acc: 0.8642\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.3833 - acc: 0.9091 - val_loss: 0.7694 - val_acc: 0.8645\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.3740 - acc: 0.9109 - val_loss: 0.7657 - val_acc: 0.8652\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.3651 - acc: 0.9124 - val_loss: 0.7667 - val_acc: 0.8657\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.3568 - acc: 0.9141 - val_loss: 0.7644 - val_acc: 0.8657\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.3486 - acc: 0.9159 - val_loss: 0.7640 - val_acc: 0.8664\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 9s 36ms/step - loss: 0.3412 - acc: 0.9172 - val_loss: 0.7651 - val_acc: 0.8660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f06a212e610>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. 번역기 구현 &모델 평가하기\n",
        "\n",
        "번역하는 과정은 크게 3가지 단계로 이루어진다.  \n",
        "\n",
        "1) 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻는다.  \n",
        "2) 인코더의 은닉 상태와 셀 상태, 그리고 토큰 `<sos>`를 디코더로 보낸다.\n",
        "3) 디코더가 토큰 `<eos>`가 나올 때까지 다음 단어를 예측하는 행동을 반복한다.\n",
        "\n",
        "인코더의 입, 출력으로 사용되는 encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용한다. 이렇게 되면 훈련 단계에 encoder_inputs와 encoder_states 사이에 있는 모든 층까지 전부 불러오게 되므로 결과적으로 훈련 단계에서 사용한 인코더를 그대로 재사용하게 된다.  \n",
        "\n",
        "디코더를 설계할 때는 테스트 단계일 때 디코더를 매 시점 별로 컨트롤 할 예정으로써, 이를 위해서 이전 시점의 상태를 저장할 텐서인 decoder_state_input_h, decoder_state_input_c를 정의한다. 매 시점 별로 디코더를 컨트롤하는 함수는 뒤에서 정의할 decode_sequence()로 해당 함수를 잘 살피는 게 중요하다."
      ],
      "metadata": {
        "id": "uBH5JWZyDc65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# 디코더 설계\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층을 재사용\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 모든 시점에 대해 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# 수정된 디코더\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)\n"
      ],
      "metadata": {
        "id": "luSsnlqqASz_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드는 테스트 단계에서의 동작을 위한 decode_sequence 함수를 구현하는 부분이다. \n",
        "\n",
        "<코드 설명>\n",
        "* 입력 문장이 들어오면 인코더는 마지막 시점까지 전개하여 마지막 시점의 은닉 상태와 셀 상태를 리턴한다.  \n",
        "* 이 두 개의 값을 states_value에 저장한다. 그리고 디코더의 초기 입력으로 `<sos>`를 준비하고 이를 target_seq에 저장한다.  \n",
        "* 이 두 가지 입력을 가지고 while문 안으로 진입하여 이 두 가지를 디코더의 입력으로 사용한다.  \n",
        "* 이제 디코더는 현재 시점에 대해서 예측을 하게 될 텐데, 이 때 현재 시점의 예측 벡터로부터 현재 시점의 예측 벡터가 output_tokens, 현재 시점의 은닉 상태를 h, 현재 시점의 셀 상태가 c이다.  \n",
        "* 예측 벡터로부터 현재 시점의 예측 단어인 target_seq를 얻고, h와 c가 두 개의 값은 states_value에 저장한다.  \n",
        "* 그리고 while문의 다음 루프 즉, 두번째 시점의 딬더의 입력으로 다시 target_seq와 states_value를 사용한다.  \n",
        "* 이를 현재 시점의 예측 단어로 `<eos>`를 예측하거나 번역 문장의 길이가 50이 넘는 순간까지 반복한다.  \n",
        "* 각 시점마다 번역된 단어는 decoded_sentence에 누적하여 저장하였다가 최종 번역 시퀀스로 리턴한다."
      ],
      "metadata": {
        "id": "eI3SYBgNGSr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>에 해당하는 정수 생성\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루프 반복\n",
        "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "  while not stop_condition:\n",
        "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 단어로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "    if (sampled_char == '<eos>' or\n",
        "        len(decoded_sentence) > 50):\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "V1NT93aEE9hO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드는 결과 확인을 위한 함수를 만드는 과정이다.\n",
        "\n",
        "<코드 설명>\n",
        "* seq_to_src 함수는 영어 문장에 해당하는 정수 시퀀스를 입력받으면 정수로부터 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환한다.  \n",
        "* seq_to_tar은 프랑스어에 해당하는 정수 시퀀스를 입력받으면 정수로부터 프랑스어 단어를 리턴하는 index_to_tar을 통해 프랑스어 문장으로 변환합니다."
      ],
      "metadata": {
        "id": "bt_3ZvzuIcil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "yDM19VulGOjM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터에 대해 임의로 선택한 인덱스의 샘플의 결과 출력\n",
        "for seq_index in [7, 45, 400, 700, 2999]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX7UYvFDIsYJ",
        "outputId": "41a1879b-e3d4-499e-8da1-04b47612b266"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : i feel frightened . \n",
            "정답문장 : je suis effrayee . \n",
            "번역문장 : je suis assez satisfait . \n",
            "--------------------------------------------------\n",
            "입력문장 : they re mine . \n",
            "정답문장 : elles sont miennes . \n",
            "번역문장 : ils sont miennes . \n",
            "--------------------------------------------------\n",
            "입력문장 : are you finished ? \n",
            "정답문장 : as tu fini ? \n",
            "번역문장 : en avez vous termine ? \n",
            "--------------------------------------------------\n",
            "입력문장 : did you find tom ? \n",
            "정답문장 : avez vous trouve tom ? \n",
            "번역문장 : avez vous tue tom ? \n",
            "--------------------------------------------------\n",
            "입력문장 : tom is your age . \n",
            "정답문장 : tom a votre age . \n",
            "번역문장 : tom est une vieille maison . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터에 대해 임의로 선택한 인덱스의 샘플의 결과 출력\n",
        "for seq_index in [7, 45, 400, 700, 2999]:\n",
        "  input_seq = encoder_input_test[seq_index: seq_index +1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print('입력문장 :', seq_to_src(encoder_input_test[seq_index]))\n",
        "  print('정답문장 :', seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print('변역문장 :', decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q92AAHLZI4YK",
        "outputId": "eb4254f4-7eab-488a-e048-44ed8ce1fc72"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : the meat is frozen . \n",
            "정답문장 : la viande est congelee . \n",
            "변역문장 : le chien est propre . \n",
            "--------------------------------------------------\n",
            "입력문장 : you are lying . \n",
            "정답문장 : vous mentez . \n",
            "변역문장 : vous etes en train de mentir . \n",
            "--------------------------------------------------\n",
            "입력문장 : can i sit down ? \n",
            "정답문장 : puis je m asseoir ? \n",
            "변역문장 : puis je m asseoir ? \n",
            "--------------------------------------------------\n",
            "입력문장 : i just feel awful . \n",
            "정답문장 : je me sens juste affreusement mal . \n",
            "변역문장 : je me suis remarie . \n",
            "--------------------------------------------------\n",
            "입력문장 : you re grounded . \n",
            "정답문장 : vous etes terre a terre . \n",
            "변역문장 : tu es mal a la . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장이 조금씩 유사한 부분이 보이는 거 같다."
      ],
      "metadata": {
        "id": "PxhvFKUiJvJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "\n",
        "이번에는 영어 문장을 단어 단위로 쪼개어 토큰화를 시킨 후 그 문장을 프랑스어로 번역해 주는 번역기를 돌리기 위한 프로젝트를 수행했다. 단어 단위로 쪼갠 번역기를 만들었다고 생각하니 너무 흥미진진했고 성능이 어떻게 나올까 연습할 때 만들었던 번역기보다 향상된 성능을 발휘할까하는 기대감 역시 들었다.\n",
        "\n",
        "번역기를 돌려보니 성능적으로도 괜찮았다고 생각이 들었다. 훈련 데이터의 경우 91.72%, 테스트 데이터의 경우 86.60% 정도로 나왔으니 말이다.\n",
        "\n",
        "이번에도 정답이라고 할 수 있는 참고가 매우 될 만한 블로그의 대부분을 인용하였기 때문에 아직까지는 코드 각각이 어떤 식으로 구성되어 있고 이런 것이 미숙할 수도 있지만 지금은 코드가 어떠한 기능이 있는지에 전체적인 이해를 하는 데에 집중키로 했다. 조만간 영한 번역기를 돌려보는 프로젝트나 익스 노드도 하지 않을까 기대를 해 본다.  "
      ],
      "metadata": {
        "id": "mlxgVp9Xa_9f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQKaNC_MefXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}